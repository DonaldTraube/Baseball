import os
import time
import pandas as pd
from selenium import webdriver
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.common.by import By
from webdriver_manager.chrome import ChromeDriverManager
from datetime import datetime, timedelta
from bs4 import BeautifulSoup

# Base URL for Batting Stats
base_url = "https://www.fangraphs.com/leaders-legacy.aspx"

# Date range
start_date = datetime(2015, 3, 31)
end_date = datetime(2015, 10, 31)

# Create output directory
output_dir = "2015_fangraphs_batting_data"
os.makedirs(output_dir, exist_ok=True)

# Set up Selenium WebDriver
chrome_options = Options()
chrome_options.add_argument("--headless")  # Run in background
chrome_options.add_argument("--disable-blink-features=AutomationControlled")
chrome_options.add_argument("--no-sandbox")
chrome_options.add_argument("--disable-dev-shm-usage")

service = Service(ChromeDriverManager().install())
driver = webdriver.Chrome(service=service, options=chrome_options)

# Loop through each date
current_date = start_date
while current_date <= end_date:
    date_str = current_date.strftime("%Y-%m-%d")
    print(f"📅 Fetching data for {date_str}...")

    # Construct URL for the specific date
    url = (
        f"{base_url}?pos=all&stats=bat&lg=all&qual=0"
        f"&type=c,336,4,6,11,12,13,21,-1,34,35,40,41,-1,23,37,38,50,317,61,-1,111,-1,203,199,58,-1,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100,101,102,103,104,105,106,107,108,109,110,111,112,113,114,115,116,117,118,119,120,121,122,123,124,125,126,127,128,129,130,131,132,133,134,135,136,137,138,139,140,141,142,143,144,145,146,147,148,149,150,151,152,153,154,155,156,157,158,159,160,161,162,163,164,165,166,167,168,169,170,171,172,173,174,175,176,177,178,179,180,181,182,183,184,185,186,187,188,189,190,191,192,193,194,195,196,197,198,199,200,201,202,203,204,205,206,207,208,209,210,211,212,213,214,215,216,217,218,219,220,221,222,223,224,225,226,227,228,229,230,231,232,233,234,235,236,237,238,239,240,241,242,243,244,245,246,247,248,249,250,251,252,253,254,255,256,257,258,259,260,261,262,263,264,265,266,267,268,269,270,271,272,273,274,275,276,277,278,279,280,281,282,283,284,285,286,287,288,289,290,291,292,293,294,295,296,297,298,299,300,301,302,303,304,305,306,307,308,309,310,311,312,313,314,315,316,317,318,319,320,321,322,323,324,325,326,327,328,329,330,331,332,333,334,335,336,337,338,339,340,341,342,343,344"
        f"&month=1000&season1=2015&ind=0&team=0,ts&rost=0&age=0&filter=&players=0"
        f"&startdate=2015-03-01&enddate={date_str}&v_cr=legacy"
    )

    try:
        driver.get(url)
        time.sleep(10)  # Wait for page to fully load

        # Find the table using the new XPath
        try:
            table_element = driver.find_element(By.XPATH, "/html/body/form/div[3]/div[3]/span[2]/div/table")
            soup = BeautifulSoup(table_element.get_attribute("outerHTML"), "html.parser")
        except:
            print(f"⚠️ Table not found for {date_str}. Skipping...")
            current_date += timedelta(days=1)
            continue

        # Extract headers
        headers_html = soup.find("thead")
        if headers_html:
            headers = [header.text.strip() for header in headers_html.find_all("th")]
        else:
            print(f"⚠️ No headers found for {date_str}. Skipping...")
            current_date += timedelta(days=1)
            continue

        # Extract rows
        rows = []
        rows_html = soup.find("tbody")
        if rows_html:
            for row in rows_html.find_all("tr"):
                row_data = [cell.text.strip() for cell in row.find_all("td")]
                rows.append(row_data)
        else:
            print(f"⚠️ No row data found for {date_str}. Skipping...")
            current_date += timedelta(days=1)
            continue

        # Add end_date column to each row
        for row in rows:
            row.append(date_str)  # Append end_date to each row

        # Add 'End Date' column to headers
        headers.append("End Date")

        # Save to CSV
        if rows:
            df = pd.DataFrame(rows, columns=headers)
            csv_filename = os.path.join(output_dir, f"{date_str}.csv")
            df.to_csv(csv_filename, index=False)
            print(f"✅ Saved: {csv_filename}")
        else:
            print(f"⚠️ No data available for {date_str}")

    except Exception as e:
        print(f"❌ Error fetching data for {date_str}: {e}")

    # Move to the next date
    current_date += timedelta(days=1)
    time.sleep(10)  # Add delay to prevent blocking

# Close the WebDriver
driver.quit()

print("🎉 Data fetching complete.")
